{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning Example - Semantic Spaces\n",
    "\n",
    "The first machine learning model we will examine in more detail belongs to the class of unsupervised models.\n",
    "Semantic spaces (also called word embeddings) are a form of machine learning models that tries to attempt to learn the relationships between words. \n",
    "\n",
    "What does this mean? While for a human it is simple to identify that speaking and spoken are very closely related and cat and dog share a relationship, this is not evident for a computer. How can we represent concepts in a form that allows computers to operate based on their meaning?\n",
    "\n",
    "Early attempts to do so (for example as part of search engines) were based on linguistic analysis of words. Identifying the common base form of verbs and plural and singular nouns allowed computer programs to \"understand\" that \"rocket\" and \"rockets\" are very closely related concepts.\n",
    "\n",
    "Modern approaches to represent the meaning of concepts are based on calculating a numeric representation (a vector with values) for concepts. Based on this approach it is possible to then to calculate how similar two concepts are by using a metric (e.g. a distance metric such as the euclidean distance or the cosine).\n",
    "Probably the most famous of these semantic word spaces is called Word2Vec ( [original Word2Vec implementation by Google](https://arxiv.org/pdf/1301.3781.pdf) ).\n",
    "The idea behind Word2Vec  is pretty simple. We are making and assumption that \"you can tell the meaning of a word by the company it keeps\" (Firth 1957 Linguist). This is analogous to the saying *show me your friends, and I'll tell who you are*. So if you have two words that have very similar neighbors (i.e. the usage context is about the same), then these words are probably quite similar in meaning or are at least highly related. For example, the words `shocked`,`appalled` and `astonished` are typically used in a similar context. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and logging\n",
    "\n",
    "First, we start with our imports and get logging established.\n",
    "\n",
    "If you are operating on a new environment you will likely miss the `gensim` package.\n",
    "`gensim` is a library specialized on Natural Language Processing methods.\n",
    "It is an open source library developed by a machine learning consulting company from the Czech Republic. \n",
    "Among other algorithms it contains a reliable implementation of `Word2Vec`.\n",
    "\n",
    "Use conda to install the gensim library on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.3 (main, May 15 2023, 10:43:03) [Clang 14.0.6 ]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "python_version = sys.version\n",
    "print(\"Python version:\", python_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed and set up logging\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset \n",
    "\n",
    "Apart from the implementation of the model, the second ingredient that is necessary for us to start with the training of unsupervised models is a dataset. \n",
    "\n",
    "You will find the following datasets in the `data` directory of the zip for today's lecture. \n",
    "\n",
    "* `swiss-sms.txt.gz` is a set of Swiss German text messages (http://www.sms4science.ch/)\n",
    "* `reviews_data.txt.gz` is a collection of  English reviews from different web sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'  Ich gang sicher nach Taipeh und entscheid no zwischen ny   und Helsinki . Ich gang ebbe scho an mim geburi nach ny  ! ! ! Yuhui \\n'\n"
     ]
    }
   ],
   "source": [
    "# to start with specify the swiss-sms.txt.gz data file below. \n",
    "data_file=\"data/swiss-sms.txt.gz\"\n",
    "\n",
    "with gzip.open (data_file, 'rb') as f:\n",
    "    for i,line in enumerate (f):\n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files into a list\n",
    "\n",
    "Now that we've had a sneak peak of our dataset, we can read it into a list so that we can pass this on to the Word2Vec model. Notice in the code below, that I am directly reading the \n",
    "compressed file. \n",
    "\n",
    "I'm also doing a mild pre-processing of the reviews using `gensim.utils.simple_preprocess (line)`. This does some basic pre-processing such as tokenization (splitting the text into individual words), lowercasing, etc and returns back a list of tokens (words). Documentation of this pre-processing method can be found on the official [Gensim documentation site](https://radimrehurek.com/gensim/utils.html). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 13:53:17,856 : INFO : reading file data/swiss-sms.txt.gz...this may take a while\n",
      "2023-06-29 13:53:17,858 : INFO : read 0 lines\n",
      "2023-06-29 13:53:17,975 : INFO : read 10000 lines\n",
      "2023-06-29 13:53:18,076 : INFO : read 20000 lines\n",
      "2023-06-29 13:53:18,175 : INFO : read 30000 lines\n",
      "2023-06-29 13:53:18,195 : INFO : Done reading data file\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "    \n",
    "    logging.info(\"reading file {0}...this may take a while\".format(input_file))\n",
    "    \n",
    "    with gzip.open (input_file, 'rb') as f:\n",
    "        for i, line in enumerate (f): \n",
    "\n",
    "            if (i%10000==0):\n",
    "                logging.info (\"read {0} lines\".format (i))\n",
    "            # do some pre-processing and return a list of words for each review text\n",
    "            yield gensim.utils.simple_preprocess (line)\n",
    "\n",
    "# read the tokenized reviews into a list\n",
    "# each review item becomes a serries of words\n",
    "# so this becomes a list of lists\n",
    "documents = list (read_input (data_file))\n",
    "logging.info (\"Done reading data file\")    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Word2Vec model\n",
    "\n",
    "Training the model is fairly straightforward. You just instantiate Word2Vec and pass the lines that we read in the previous step (the `documents`). So, we are essentially passing on a list of lists. Where each list within the main list contains a set of tokens from a user review. Word2Vec uses all these tokens to internally create a vocabulary. And by vocabulary, I mean a set of unique words.\n",
    "\n",
    "After building the vocabulary, we just need to call `train(...)` to start training the Word2Vec model. \n",
    "Training time depends on the size of the training data. \n",
    "The Swiss Text dataset is rather small and should train very quickly.\n",
    "\n",
    "Behind the scenes we are actually training a simple neural network with a single hidden layer. But, we are actually not going to use the neural network after training. Instead, the goal is to learn the weights of the hidden layer. These weights are essentially the word vectors that weâ€™re trying to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 13:54:04,418 : INFO : collecting all words and their counts\n",
      "2023-06-29 13:54:04,419 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-06-29 13:54:04,437 : INFO : PROGRESS: at sentence #10000, processed 92380 words, keeping 9231 word types\n",
      "2023-06-29 13:54:04,453 : INFO : PROGRESS: at sentence #20000, processed 185072 words, keeping 14705 word types\n",
      "2023-06-29 13:54:04,468 : INFO : PROGRESS: at sentence #30000, processed 274733 words, keeping 19454 word types\n",
      "2023-06-29 13:54:04,471 : INFO : collected 20302 word types from a corpus of 293200 raw words and 32039 sentences\n",
      "2023-06-29 13:54:04,472 : INFO : Creating a fresh vocabulary\n",
      "2023-06-29 13:54:04,479 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=15 retains 2076 unique words (10.23% of original 20302, drops 18226)', 'datetime': '2023-06-29T13:54:04.479334', 'gensim': '4.3.0', 'python': '3.11.3 (main, May 15 2023, 10:43:03) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-06-29 13:54:04,479 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=15 leaves 234238 word corpus (79.89% of original 293200, drops 58962)', 'datetime': '2023-06-29T13:54:04.479848', 'gensim': '4.3.0', 'python': '3.11.3 (main, May 15 2023, 10:43:03) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-06-29 13:54:04,486 : INFO : deleting the raw counts dictionary of 20302 items\n",
      "2023-06-29 13:54:04,487 : INFO : sample=0.001 downsamples 71 most-common words\n",
      "2023-06-29 13:54:04,487 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 190823.74620005421 word corpus (81.5%% of prior 234238)', 'datetime': '2023-06-29T13:54:04.487663', 'gensim': '4.3.0', 'python': '3.11.3 (main, May 15 2023, 10:43:03) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-06-29 13:54:04,498 : INFO : estimated required memory for 2076 words and 150 dimensions: 3529200 bytes\n",
      "2023-06-29 13:54:04,498 : INFO : resetting layer weights\n",
      "2023-06-29 13:54:04,501 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-06-29T13:54:04.501521', 'gensim': '4.3.0', 'python': '3.11.3 (main, May 15 2023, 10:43:03) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2023-06-29 13:54:04,501 : INFO : Word2Vec lifecycle event {'msg': 'training model with 10 workers on 2076 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2023-06-29T13:54:04.501805', 'gensim': '4.3.0', 'python': '3.11.3 (main, May 15 2023, 10:43:03) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2023-06-29 13:54:04,594 : INFO : EPOCH 0: training on 293200 raw words (190848 effective words) took 0.1s, 2277792 effective words/s\n",
      "2023-06-29 13:54:04,687 : INFO : EPOCH 1: training on 293200 raw words (190927 effective words) took 0.1s, 2265422 effective words/s\n",
      "2023-06-29 13:54:04,802 : INFO : EPOCH 2: training on 293200 raw words (190892 effective words) took 0.1s, 1798582 effective words/s\n",
      "2023-06-29 13:54:04,895 : INFO : EPOCH 3: training on 293200 raw words (190852 effective words) took 0.1s, 2255370 effective words/s\n",
      "2023-06-29 13:54:04,987 : INFO : EPOCH 4: training on 293200 raw words (190831 effective words) took 0.1s, 2275925 effective words/s\n",
      "2023-06-29 13:54:04,987 : INFO : Word2Vec lifecycle event {'msg': 'training on 1466000 raw words (954350 effective words) took 0.5s, 1963726 effective words/s', 'datetime': '2023-06-29T13:54:04.987982', 'gensim': '4.3.0', 'python': '3.11.3 (main, May 15 2023, 10:43:03) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2023-06-29 13:54:04,988 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=2076, vector_size=150, alpha=0.025>', 'datetime': '2023-06-29T13:54:04.988287', 'gensim': '4.3.0', 'python': '3.11.3 (main, May 15 2023, 10:43:03) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "2023-06-29 13:54:04,988 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2023-06-29 13:54:04,988 : INFO : Word2Vec lifecycle event {'msg': 'training model with 10 workers on 2076 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2023-06-29T13:54:04.988895', 'gensim': '4.3.0', 'python': '3.11.3 (main, May 15 2023, 10:43:03) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2023-06-29 13:54:05,083 : INFO : EPOCH 0: training on 293200 raw words (190902 effective words) took 0.1s, 2240749 effective words/s\n",
      "2023-06-29 13:54:05,175 : INFO : EPOCH 1: training on 293200 raw words (191027 effective words) took 0.1s, 2300325 effective words/s\n",
      "2023-06-29 13:54:05,272 : INFO : EPOCH 2: training on 293200 raw words (190826 effective words) took 0.1s, 2156903 effective words/s\n",
      "2023-06-29 13:54:05,370 : INFO : EPOCH 3: training on 293200 raw words (190792 effective words) took 0.1s, 2158281 effective words/s\n",
      "2023-06-29 13:54:05,467 : INFO : EPOCH 4: training on 293200 raw words (190684 effective words) took 0.1s, 2180951 effective words/s\n",
      "2023-06-29 13:54:05,560 : INFO : EPOCH 5: training on 293200 raw words (190859 effective words) took 0.1s, 2263035 effective words/s\n",
      "2023-06-29 13:54:05,652 : INFO : EPOCH 6: training on 293200 raw words (191004 effective words) took 0.1s, 2272531 effective words/s\n",
      "2023-06-29 13:54:05,751 : INFO : EPOCH 7: training on 293200 raw words (191007 effective words) took 0.1s, 2373778 effective words/s\n",
      "2023-06-29 13:54:05,845 : INFO : EPOCH 8: training on 293200 raw words (190946 effective words) took 0.1s, 2235623 effective words/s\n",
      "2023-06-29 13:54:05,935 : INFO : EPOCH 9: training on 293200 raw words (190962 effective words) took 0.1s, 2342755 effective words/s\n",
      "2023-06-29 13:54:05,936 : INFO : Word2Vec lifecycle event {'msg': 'training on 2932000 raw words (1909009 effective words) took 0.9s, 2015811 effective words/s', 'datetime': '2023-06-29T13:54:05.936202', 'gensim': '4.3.0', 'python': '3.11.3 (main, May 15 2023, 10:43:03) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1909009, 2932000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec (documents, vector_size=150, window=10, min_count=15, workers=10)\n",
    "model.train(documents,total_examples=len(documents),epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Semantic Space\n",
    "\n",
    "This first example shows a simple case of looking up the most similar words. All we need to do here is to call the `most_similar` function and provide a word as input. This returns the top 10 similar words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gahne', 0.634061336517334),\n",
       " ('gÃ¶mmer', 0.6324822306632996),\n",
       " ('gangi', 0.6123520135879517),\n",
       " ('fitness', 0.5987676978111267),\n",
       " ('gÃ¶nd', 0.5409850478172302),\n",
       " ('chum', 0.5386267900466919),\n",
       " ('kondi', 0.5122840404510498),\n",
       " ('gahni', 0.5122436881065369),\n",
       " ('poschte', 0.5094054341316223),\n",
       " ('warschindli', 0.49837127327919006)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w1 = \"gang\"\n",
    "model.wv.most_similar (positive=w1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good, right? Let's look at a few more. As you can see below the `topn` parameter specifies the number of similar words to return. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'finito' is not present in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "# look up top 6 words similar to 'polite'\n",
    "if 'finito' in model.wv:\n",
    "    similar_words = model.wv.most_similar(positive=['finito'], topn=6)\n",
    "    # Process similar_words as needed\n",
    "else:\n",
    "    print(\"'finito' is not present in the vocabulary.\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's, nice. You can even specify several positive examples to get things that are related in the provided context and provide negative examples to say what should not be considered as related. In the example below we are asking for all items that *relate to bed* only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some negative words are not present in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "# get everything related to stuff on the bed\n",
    "positive_words = ['gangi', 'gahne']\n",
    "negative_words = ['fuessball']\n",
    "\n",
    "# Check if all positive words are in vocabulary\n",
    "if all(word in model.wv for word in positive_words):\n",
    "    # Check if all negative words are in vocabulary\n",
    "    if all(word in model.wv for word in negative_words):\n",
    "        similar_words = model.wv.most_similar(positive=positive_words, negative=negative_words, topn=10)\n",
    "        # Process similar_words as needed\n",
    "    else:\n",
    "        print(\"Some negative words are not present in the vocabulary.\")\n",
    "else:\n",
    "    print(\"Some positive words are not present in the vocabulary.\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity between two words in the vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even use the Word2Vec model to return the similarity between two words that are present in the vocabulary. \n",
    "This is the basic usage of the representations of the concepts.\n",
    "Having a numerical representation allows us to compute the similarity between any two words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42137772"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two different words\n",
    "model.wv.similarity(w1=\"gangi\",w2=\"gahne\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two identical words\n",
    "model.wv.similarity(w1=\"gangi\",w2=\"gangi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.040978428"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two unrelated words\n",
    "model.wv.similarity(w1=\"gangi\",w2=\"dr\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the above three snippets computes the cosine similarity between the two specified words using word vectors of each. If you do a similarity between two identical words, the score will be 1.0 as the range of the cosine similarity score will always be between [0.0-1.0]. You can read more about cosine similarity scoring [here](https://en.wikipedia.org/wiki/Cosine_similarity)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the odd one out\n",
    "You can even use Word2Vec to find odd items given a list of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dr'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"gangi\",\"gahne\",\"dr\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding some of the (hyper-) parameters\n",
    "To train the model earlier, we had to set some hyperparameters. Now, let's try to understand what some of them mean. For reference, this is the command that we used to train the model.\n",
    "\n",
    "```\n",
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)\n",
    "```\n",
    "\n",
    "### `size`\n",
    "The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes. 100-150 dimensions are common dimensions. \n",
    "\n",
    "### `window`\n",
    "The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give allow you to strengthen relationships that are synonymous, while larger window sizes favor associative relationships.\n",
    "\n",
    "    The distinction between synonymous and associative relationships is based on findings in cognitive linguistics. Based on word priming experimentation, two main relations between words have been identified (see [CHIA1990]): synonymous relations (also referred to as similar or semantic relations in the cognitive science literature) and associative relations. As outlined in [CHIA1990], the distinction for both relationship types is not exclusive; that is, word relations are not exclusively synonymous or associative. Doctor - Nurse is an example of a word relation that can be considered as being of a synonymous-associative nature.\n",
    "\n",
    "\n",
    "    Two terms/words are associatively related if they often appear in a shared context. The following are examples of this type of relationship:\n",
    "\n",
    "            Spider - Web\n",
    "            Google - Page rank\n",
    "            Smoke - Cigarette\n",
    "            Phone - Call\n",
    "            Lighter - Fire\n",
    "\n",
    "    Two terms/words are synonymously related if they share common features. The following are examples of this type of relationship:\n",
    "\n",
    "            Wolf - Dog\n",
    "            Cetacean - Whale\n",
    "            Astronaut - Man\n",
    "            Car - Van\n",
    "            Smartphone - iPhone 4s\n",
    "\n",
    "[CHIA1990]\t(1, 2) Chiarello, Christine, et al. Semantic and associative priming in the cerebral hemispheres: Some words do, some words donâ€™t... sometimes, some places. Brain and language 38.1 (1990): 75-104\n",
    "\n",
    "\n",
    "### `min_count`\n",
    "Minimium frequency count of words. The model would ignore words that do not statisfy the `min_count`. Extremely infrequent words are usually unimportant (spelling mistakes, non-words), so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.\n",
    "\n",
    "### `workers`\n",
    "How many threads to use behind the scenes?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - Hyperparameters\n",
    "\n",
    "\n",
    "Use the hyperparameters to try to improve generating semantic spaces (word embeddings) based on the Swiss SMS corpus. Follow the steps below to generate different trained word2vec models.\n",
    "\n",
    "* Change the hyperparameters\n",
    "* Train a new semantic space\n",
    "* Explore your newly trained semantic space\n",
    "\n",
    "Changing hyperparameters is one way to change the behavior of the training.\n",
    "Hyperparameters are meant as tools to adapt the training process to the nature, context and amount of training data. It should become clear that it is a great advantage if the training process is fast (as in does not take a lot of time). \n",
    "\n",
    "You should address the following questions:\n",
    "\n",
    "* What hyperparameters impact the training the most? Try to get an understanding of the effect of the hyperparameters. How do the results change if you adjust the available hyperparameters. \n",
    "* Compare some of your trained semantic spaces. What is your setting that works best for the SMS dataset?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 13:56:31,265 : INFO : collecting all words and their counts\n",
      "2023-06-29 13:56:31,266 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-06-29 13:56:31,283 : INFO : PROGRESS: at sentence #10000, processed 92380 words, keeping 9231 word types\n",
      "2023-06-29 13:56:31,299 : INFO : PROGRESS: at sentence #20000, processed 185072 words, keeping 14705 word types\n",
      "2023-06-29 13:56:31,314 : INFO : PROGRESS: at sentence #30000, processed 274733 words, keeping 19454 word types\n",
      "2023-06-29 13:56:31,317 : INFO : collected 20302 word types from a corpus of 293200 raw words and 32039 sentences\n",
      "2023-06-29 13:56:31,318 : INFO : Creating a fresh vocabulary\n",
      "2023-06-29 13:56:31,345 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 14833 unique words (73.06% of original 20302, drops 5469)', 'datetime': '2023-06-29T13:56:31.345009', 'gensim': '4.3.0', 'python': '3.11.3 (main, May 15 2023, 10:43:03) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-06-29 13:56:31,345 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 287731 word corpus (98.13% of original 293200, drops 5469)', 'datetime': '2023-06-29T13:56:31.345488', 'gensim': '4.3.0', 'python': '3.11.3 (main, May 15 2023, 10:43:03) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-06-29 13:56:31,383 : INFO : deleting the raw counts dictionary of 20302 items\n",
      "2023-06-29 13:56:31,384 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2023-06-29 13:56:31,385 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 251540.58277706135 word corpus (87.4%% of prior 287731)', 'datetime': '2023-06-29T13:56:31.384988', 'gensim': '4.3.0', 'python': '3.11.3 (main, May 15 2023, 10:43:03) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-06-29 13:56:31,449 : INFO : estimated required memory for 14833 words and 150 dimensions: 25216100 bytes\n",
      "2023-06-29 13:56:31,450 : INFO : resetting layer weights\n",
      "2023-06-29 13:56:31,460 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-06-29T13:56:31.460275', 'gensim': '4.3.0', 'python': '3.11.3 (main, May 15 2023, 10:43:03) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2023-06-29 13:56:31,460 : INFO : Word2Vec lifecycle event {'msg': 'training model with 10 workers on 14833 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2023-06-29T13:56:31.460618', 'gensim': '4.3.0', 'python': '3.11.3 (main, May 15 2023, 10:43:03) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2023-06-29 13:56:31,586 : INFO : EPOCH 0: training on 293200 raw words (251428 effective words) took 0.1s, 2151991 effective words/s\n",
      "2023-06-29 13:56:31,714 : INFO : EPOCH 1: training on 293200 raw words (251554 effective words) took 0.1s, 2104421 effective words/s\n",
      "2023-06-29 13:56:31,848 : INFO : EPOCH 2: training on 293200 raw words (251438 effective words) took 0.1s, 2014183 effective words/s\n",
      "2023-06-29 13:56:31,987 : INFO : EPOCH 3: training on 293200 raw words (251468 effective words) took 0.1s, 1948634 effective words/s\n",
      "2023-06-29 13:56:32,131 : INFO : EPOCH 4: training on 293200 raw words (251590 effective words) took 0.1s, 1963435 effective words/s\n",
      "2023-06-29 13:56:32,131 : INFO : Word2Vec lifecycle event {'msg': 'training on 1466000 raw words (1257478 effective words) took 0.7s, 1874562 effective words/s', 'datetime': '2023-06-29T13:56:32.131716', 'gensim': '4.3.0', 'python': '3.11.3 (main, May 15 2023, 10:43:03) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2023-06-29 13:56:32,132 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=14833, vector_size=150, alpha=0.025>', 'datetime': '2023-06-29T13:56:32.132076', 'gensim': '4.3.0', 'python': '3.11.3 (main, May 15 2023, 10:43:03) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "2023-06-29 13:56:32,132 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2023-06-29 13:56:32,133 : INFO : Word2Vec lifecycle event {'msg': 'training model with 10 workers on 14833 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2023-06-29T13:56:32.133294', 'gensim': '4.3.0', 'python': '3.11.3 (main, May 15 2023, 10:43:03) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2023-06-29 13:56:32,251 : INFO : EPOCH 0: training on 293200 raw words (251584 effective words) took 0.1s, 2319423 effective words/s\n",
      "2023-06-29 13:56:32,374 : INFO : EPOCH 1: training on 293200 raw words (251217 effective words) took 0.1s, 2191905 effective words/s\n",
      "2023-06-29 13:56:32,505 : INFO : EPOCH 2: training on 293200 raw words (251639 effective words) took 0.1s, 2080374 effective words/s\n",
      "2023-06-29 13:56:32,626 : INFO : EPOCH 3: training on 293200 raw words (251689 effective words) took 0.1s, 2227790 effective words/s\n",
      "2023-06-29 13:56:32,748 : INFO : EPOCH 4: training on 293200 raw words (251584 effective words) took 0.1s, 2218242 effective words/s\n",
      "2023-06-29 13:56:32,866 : INFO : EPOCH 5: training on 293200 raw words (251257 effective words) took 0.1s, 2311094 effective words/s\n",
      "2023-06-29 13:56:32,983 : INFO : EPOCH 6: training on 293200 raw words (251262 effective words) took 0.1s, 2330632 effective words/s\n",
      "2023-06-29 13:56:33,103 : INFO : EPOCH 7: training on 293200 raw words (251562 effective words) took 0.1s, 2272353 effective words/s\n",
      "2023-06-29 13:56:33,224 : INFO : EPOCH 8: training on 293200 raw words (251655 effective words) took 0.1s, 2244672 effective words/s\n",
      "2023-06-29 13:56:33,347 : INFO : EPOCH 9: training on 293200 raw words (251505 effective words) took 0.1s, 2199254 effective words/s\n",
      "2023-06-29 13:56:33,347 : INFO : Word2Vec lifecycle event {'msg': 'training on 2932000 raw words (2514954 effective words) took 1.2s, 2071673 effective words/s', 'datetime': '2023-06-29T13:56:33.347630', 'gensim': '4.3.0', 'python': '3.11.3 (main, May 15 2023, 10:43:03) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add hyperparameter\n",
    "model = gensim.models.Word2Vec (documents, vector_size=150, window=10, min_count=2, workers=10)\n",
    "model.train(documents,total_examples=len(documents),epochs=10)\n",
    "\n",
    "# similarity between two different words\n",
    "model.wv.similarity(w1=\"gangi\",w2=\"gahne\")\n",
    "\n",
    "# similarity between two identical words\n",
    "model.wv.similarity(w1=\"gangi\",w2=\"gangi\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - Training Data\n",
    "\n",
    "\n",
    "## Train with Reviews Data\n",
    "\n",
    "There is a second dataset available for unsupervised training. \n",
    "\n",
    "    reviews_data.txt.gz\n",
    "\n",
    "This dataset consists of review data collected from a variety of sources.\n",
    "You can find details here: http://kavita-ganesan.com/entity-ranking-data/\n",
    "\n",
    "## More Data -> Better Results\n",
    "\n",
    "Adjusting the data used for training is the second major possibility for optimising models we train.\n",
    "Generally it can be said that:\n",
    "* more data -> better results\n",
    "* higher quality data -> better results\n",
    "\n",
    "\n",
    "\n",
    "However the **domain** of the data must fit our goals.\n",
    "E.g. review data might be very good to train the meaning of adjectives but less viable to train vocabulary relating to engineering. \n",
    "\n",
    "## Re-visiting Hyperparameter Effect\n",
    "\n",
    "Sometimes the effect of hyperparameters can only be observed if we have enough high quality training data.\n",
    "Training on too small datasets will lead to 'random' results and lead to 'random' observed impact of hyperparameters.\n",
    "\n",
    "Given the bigger dataset, please re-visit the analysis we did before.\n",
    "\n",
    "* Switch to the new dataset and start evaluating the effect of the following two hyperparameters:\n",
    "   * window\n",
    "   * min_count\n",
    "   \n",
    "Having a larger dataset should allow us to make ***more reliable*** observations.\n",
    "\n",
    "## Saving Models to Disk\n",
    "\n",
    "When training these larger models we should also start to write the training output to disk.\n",
    "\n",
    "You can use: `model.wv.save_word2vec_format(path)` function of the model to do so.\n",
    "\n",
    "### Versioning of Models\n",
    "\n",
    "Machine learning is a game of complexity.\n",
    "Even in the most simplistic setups you will usually deal with quite a range of factors (algorithm implementations, large amount of used packages, pre-processing pipelines, training material).\n",
    "\n",
    "It is therefore ***very important*** to get a habit of clean and exact house-keeping from the beginning on.\n",
    "\n",
    "At the least your setup should document and version:\n",
    "\n",
    "* Training Material Used (i.e. what collection, e.g. timestamped, S3 bucket)\n",
    "* Codebase Used (git tag or commit hash)\n",
    "* Hyperparameters used\n",
    "\n",
    "It can be handy to encode some of this information into the serialised model name as shown below. \n",
    "\n",
    "E.g. `model.wv.save_word2vec_format('./word2vec_swiss_text_d150_ww10_min2')`\n",
    "\n",
    "\n",
    "When training with different hyperparameters during this exercise please save the resulting models. \n",
    "We will need them for the next notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 13:57:17,758 : INFO : storing 14833x150 projection weights into ./word2vec_swiss_text_d150_ww10_min2\n"
     ]
    }
   ],
   "source": [
    "# save model to disk e.g. `model.wv.save_word2vec_format('./word2vec_swiss_text_d150_ww10_min2')`\n",
    "model.wv.save_word2vec_format('./word2vec_swiss_text_d150_ww10_min2')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
